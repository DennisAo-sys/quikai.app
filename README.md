**project-quik-v2-instruct-4b-coolai**

<hr>

**Introduction**

In recent years, advances in artificial intelligence (AI) and natural language processing (NLP) have begun to radically change user experience and business processes. One of the representatives of this revolutionary transformation is the Quik model.

Quik was basically developed on the 1.5 billion parameter version of GPT-2 (GPT-2 1.5b). This model, whose pre-training phase was completed by the OpenAI company, was trained with dense data sets and transformed into a large model with a total of 4 billion parameters. In addition, four different distilled versions of the model (4bit, 8bit, 16bit and 32bit) were prepared to optimize it according to the field of use.

<hr>

**Updated Architecture with New Generation Techniques**

Technical innovations that have recently been found important in the world of Transformers and NLP have been added to the GPT-2 based model. These include rotary positional embeddings (RoPE), multi-query attention (MQA), FlashAttention and long-context techniques. These innovations have taken the language understanding and production capabilities of the Quik model to the next level, while also increasing its processing efficiency and accuracy.

<hr>

**Training Process**

The Quik model was initially trained using a large training set containing billions of data. These data sets include:
<ul>
<li>The identity of the Quik model.</li>
<li>Social Development and Solidarity Association (SGDD-ASAM) migration data and terms, United Nations organization terms, articles and documents on the concept of migration (Google Scholar, Semantic Scholar).</li>
<li>Open source data on coding and software (Stack Overflow, GitHub, Kaggle).</li>
<li>Mathematics, data analytics and scientific articles.</li>
<li>Wikipedia and academic literature.</li>
<li>News sites, blogs, dictionaries, technology platforms and various websites.</li>
</ul>

The data sets are organized according to the behaviors of the Quik model and prepared in Alpaca and ChatML formats

**1) Alpaca**: It is a simple and effective JSON-based data structure used to train text-based large language models (LLMs) with the instruction tuning method. Each data sample consists of the following three main fields to ensure that the model produces a meaningful and task-appropriate output for an input (e.g. a question or instruction):
<ul>
<li><em>instruction</em>: A natural language description of the task requested from the model.</li>
<li><em>input</em>: Contains additional context or data related to the task.</li>
<li><em>output</em>: The correct or ideal output expected from the model.</li>
</ul>

**2) ChatML**: A data format developed by OpenAI and used to train chat-based large language models (LLMs). This format allows the model to understand cross-role dialogues and produce more controlled responses. It consists of the following three main structures to establish clear dialogues in conversational models:
<ul>
<li><em>system</em>: An instruction that defines the general behavior and role of the model.</li>
<li><em>user</em>: A user message asking the model to perform a specific task.</li>
<li><em>assistant</em>: The response to the user's request; in other words, the output produced by the model.</li>
</ul>

<hr>
